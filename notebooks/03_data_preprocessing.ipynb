{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08c8c6c0",
   "metadata": {},
   "source": [
    "# 03 Data Preprocessing\n",
    "\n",
    "Create train/val/test splits and data loaders for model training.\n",
    "\n",
    "**Outputs:**\n",
    "- Split indices saved to JSON for reproducibility\n",
    "- DataLoader factory for training notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c0e67e",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c36297f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded\n",
      "  Data directory: ..\\data\\CleanPetImages\n",
      "  Subset mode: True (4000 images)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# All configuration in one place\n",
    "config = {\n",
    "    # Paths\n",
    "    'data_dir': Path('../data/CleanPetImages'),\n",
    "    'output_dir': Path('../outputs'),\n",
    "    'split_file': Path('../outputs/splits/train_val_test_split.json'),\n",
    "    \n",
    "    # Split ratios (val and test are from total, train is remainder)\n",
    "    'val_ratio': 0.10,\n",
    "    'test_ratio': 0.10,\n",
    "    \n",
    "    # Iteration subset - use smaller training set for fast experiments\n",
    "    'use_subset': True,\n",
    "    'subset_size': 4000,\n",
    "    \n",
    "    # DataLoader settings\n",
    "    'batch_size': 32,\n",
    "    'num_workers': 4,\n",
    "    'pin_memory': True,\n",
    "    \n",
    "    # Image settings\n",
    "    'image_size': 224,\n",
    "    \n",
    "    # Reproducibility\n",
    "    'random_seed': 42,\n",
    "}\n",
    "\n",
    "# Create output directories\n",
    "config['output_dir'].mkdir(parents=True, exist_ok=True)\n",
    "(config['output_dir'] / 'splits').mkdir(exist_ok=True)\n",
    "\n",
    "print(\"Configuration loaded\")\n",
    "print(f\"  Data directory: {config['data_dir']}\")\n",
    "print(f\"  Subset mode: {config['use_subset']} ({config['subset_size']} images)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3abd97f8",
   "metadata": {},
   "source": [
    "## Load File Paths\n",
    "\n",
    "Load all image paths from the clean dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "181ef8e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 24924 images\n",
      "  Cats: 12456\n",
      "  Dogs: 12468\n"
     ]
    }
   ],
   "source": [
    "def load_file_paths(data_dir):\n",
    "    \"\"\"\n",
    "    Load all image paths from Cat and Dog folders.\n",
    "    \n",
    "    Returns:\n",
    "        files: list of paths relative to data_dir (e.g., 'Cat/123.jpg')\n",
    "        labels: list of int labels (0=cat, 1=dog)\n",
    "    \"\"\"\n",
    "    data_dir = Path(data_dir)\n",
    "    \n",
    "    cat_files = sorted(data_dir.glob('Cat/*.jpg'))\n",
    "    dog_files = sorted(data_dir.glob('Dog/*.jpg'))\n",
    "    \n",
    "    # Store as relative paths (portable across machines)\n",
    "    files = []\n",
    "    labels = []\n",
    "    \n",
    "    for f in cat_files:\n",
    "        files.append(f'Cat/{f.name}')\n",
    "        labels.append(0)\n",
    "    \n",
    "    for f in dog_files:\n",
    "        files.append(f'Dog/{f.name}')\n",
    "        labels.append(1)\n",
    "    \n",
    "    return files, labels\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "all_files, all_labels = load_file_paths(config['data_dir'])\n",
    "\n",
    "# Quick summary\n",
    "n_cats = sum(1 for label in all_labels if label == 0)\n",
    "n_dogs = sum(1 for label in all_labels if label == 1)\n",
    "\n",
    "print(f\"Loaded {len(all_files)} images\")\n",
    "print(f\"  Cats: {n_cats}\")\n",
    "print(f\"  Dogs: {n_dogs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4121f7d",
   "metadata": {},
   "source": [
    "## Train/Val/Test Split\n",
    "\n",
    "Split the dataset with stratification to maintain class balance.\n",
    "\n",
    "- Train: 80% (used for model training)\n",
    "- Val: 10% (used for hyperparameter tuning)  \n",
    "- Test: 10% (held out for final evaluation)\n",
    "\n",
    "Val and test sets are fixed across all experiments. Only train set size changes when using subset mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2165fc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split sizes:\n",
      "  Train: 19938\n",
      "  Val:   2493\n",
      "  Test:  2493\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "def create_splits(files, labels, val_ratio, test_ratio, random_seed):\n",
    "    \"\"\"\n",
    "    Create stratified train/val/test splits.\n",
    "    \n",
    "    Args:\n",
    "        files: list of file paths\n",
    "        labels: list of labels (0 or 1)\n",
    "        val_ratio: fraction for validation (e.g., 0.10)\n",
    "        test_ratio: fraction for test (e.g., 0.10)\n",
    "        random_seed: for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "        dict with train_files, val_files, test_files, and corresponding labels\n",
    "    \"\"\"\n",
    "    # First split: separate test set\n",
    "    train_val_files, test_files, train_val_labels, test_labels = train_test_split(\n",
    "        files, \n",
    "        labels,\n",
    "        test_size=test_ratio,\n",
    "        stratify=labels,\n",
    "        random_state=random_seed\n",
    "    )\n",
    "    \n",
    "    # Second split: separate val from train\n",
    "    # Adjust val_ratio since we're splitting from (1 - test_ratio)\n",
    "    adjusted_val_ratio = val_ratio / (1 - test_ratio)\n",
    "    \n",
    "    train_files, val_files, train_labels, val_labels = train_test_split(\n",
    "        train_val_files,\n",
    "        train_val_labels,\n",
    "        test_size=adjusted_val_ratio,\n",
    "        stratify=train_val_labels,\n",
    "        random_state=random_seed\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'train_files': train_files,\n",
    "        'train_labels': train_labels,\n",
    "        'val_files': val_files,\n",
    "        'val_labels': val_labels,\n",
    "        'test_files': test_files,\n",
    "        'test_labels': test_labels,\n",
    "    }\n",
    "\n",
    "\n",
    "# Create the splits\n",
    "splits = create_splits(\n",
    "    all_files, \n",
    "    all_labels,\n",
    "    val_ratio=config['val_ratio'],\n",
    "    test_ratio=config['test_ratio'],\n",
    "    random_seed=config['random_seed']\n",
    ")\n",
    "\n",
    "print(\"Split sizes:\")\n",
    "print(f\"  Train: {len(splits['train_files'])}\")\n",
    "print(f\"  Val:   {len(splits['val_files'])}\")\n",
    "print(f\"  Test:  {len(splits['test_files'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "edf4b802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution (should be ~50/50 in each):\n",
      "\n",
      "Train:\n",
      "  Cats: 9964 (50.0%)\n",
      "  Dogs: 9974 (50.0%)\n",
      "\n",
      "Val:\n",
      "  Cats: 1246 (50.0%)\n",
      "  Dogs: 1247 (50.0%)\n",
      "\n",
      "Test:\n",
      "  Cats: 1246 (50.0%)\n",
      "  Dogs: 1247 (50.0%)\n"
     ]
    }
   ],
   "source": [
    "def print_class_distribution(labels, name):\n",
    "    \"\"\"Print class balance for a split.\"\"\"\n",
    "    labels = np.array(labels)\n",
    "    n_cats = np.sum(labels == 0)\n",
    "    n_dogs = np.sum(labels == 1)\n",
    "    total = len(labels)\n",
    "    \n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Cats: {n_cats} ({100*n_cats/total:.1f}%)\")\n",
    "    print(f\"  Dogs: {n_dogs} ({100*n_dogs/total:.1f}%)\")\n",
    "\n",
    "\n",
    "# Verify stratification maintained class balance\n",
    "print(\"Class distribution (should be ~50/50 in each):\\n\")\n",
    "print_class_distribution(splits['train_labels'], \"Train\")\n",
    "print()\n",
    "print_class_distribution(splits['val_labels'], \"Val\")\n",
    "print()\n",
    "print_class_distribution(splits['test_labels'], \"Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f8873f",
   "metadata": {},
   "source": [
    "## Create Iteration Subset\n",
    "\n",
    "Select a fixed subset of training data for fast hyperparameter iteration.\n",
    "The subset is stratified to maintain class balance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e721e76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset size: 3999\n",
      "\n",
      "Subset:\n",
      "  Cats: 1998 (50.0%)\n",
      "  Dogs: 2001 (50.0%)\n"
     ]
    }
   ],
   "source": [
    "def create_subset_indices(train_labels, subset_size, random_seed):\n",
    "    \"\"\"\n",
    "    Select stratified subset indices from training set.\n",
    "    \n",
    "    Args:\n",
    "        train_labels: labels for training set\n",
    "        subset_size: number of samples for subset\n",
    "        random_seed: for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "        list of indices into the training set\n",
    "    \"\"\"\n",
    "    n_train = len(train_labels)\n",
    "    \n",
    "    if subset_size >= n_train:\n",
    "        print(f\"Subset size ({subset_size}) >= train size ({n_train}), using all training data\")\n",
    "        return list(range(n_train))\n",
    "    \n",
    "    # Use train_test_split to get stratified subset\n",
    "    all_indices = list(range(n_train))\n",
    "    subset_ratio = subset_size / n_train\n",
    "    \n",
    "    subset_indices, _ = train_test_split(\n",
    "        all_indices,\n",
    "        train_size=subset_ratio,\n",
    "        stratify=train_labels,\n",
    "        random_state=random_seed\n",
    "    )\n",
    "    \n",
    "    return sorted(subset_indices)\n",
    "\n",
    "\n",
    "# Create subset indices\n",
    "subset_indices = create_subset_indices(\n",
    "    splits['train_labels'],\n",
    "    config['subset_size'],\n",
    "    config['random_seed']\n",
    ")\n",
    "\n",
    "# Verify subset\n",
    "subset_labels = [splits['train_labels'][i] for i in subset_indices]\n",
    "print(f\"Subset size: {len(subset_indices)}\")\n",
    "print()\n",
    "print_class_distribution(subset_labels, \"Subset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d652ed9",
   "metadata": {},
   "source": [
    "## Save Splits to JSON\n",
    "\n",
    "Save all split information for reproducibility across notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5be708d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splits saved to ..\\outputs\\splits\\train_val_test_split.json\n"
     ]
    }
   ],
   "source": [
    "def save_splits(splits, subset_indices, config):\n",
    "    \"\"\"Save splits to JSON file.\"\"\"\n",
    "    \n",
    "    split_data = {\n",
    "        'train_files': splits['train_files'],\n",
    "        'train_labels': splits['train_labels'],\n",
    "        'val_files': splits['val_files'],\n",
    "        'val_labels': splits['val_labels'],\n",
    "        'test_files': splits['test_files'],\n",
    "        'test_labels': splits['test_labels'],\n",
    "        'subset_indices': subset_indices,\n",
    "        'config': {\n",
    "            'val_ratio': config['val_ratio'],\n",
    "            'test_ratio': config['test_ratio'],\n",
    "            'subset_size': config['subset_size'],\n",
    "            'random_seed': config['random_seed'],\n",
    "        },\n",
    "        'created': datetime.now().isoformat(),\n",
    "    }\n",
    "    \n",
    "    with open(config['split_file'], 'w') as f:\n",
    "        json.dump(split_data, f, indent=2)\n",
    "    \n",
    "    print(f\"Splits saved to {config['split_file']}\")\n",
    "\n",
    "\n",
    "# Save\n",
    "save_splits(splits, subset_indices, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b474bf2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verification - reloaded from JSON:\n",
      "  Train files: 19938\n",
      "  Val files:   2493\n",
      "  Test files:  2493\n",
      "  Subset indices: 3999\n",
      "  Random seed: 42\n"
     ]
    }
   ],
   "source": [
    "# Quick verification - reload and check\n",
    "with open(config['split_file'], 'r') as f:\n",
    "    loaded = json.load(f)\n",
    "\n",
    "print(\"Verification - reloaded from JSON:\")\n",
    "print(f\"  Train files: {len(loaded['train_files'])}\")\n",
    "print(f\"  Val files:   {len(loaded['val_files'])}\")\n",
    "print(f\"  Test files:  {len(loaded['test_files'])}\")\n",
    "print(f\"  Subset indices: {len(loaded['subset_indices'])}\")\n",
    "print(f\"  Random seed: {loaded['config']['random_seed']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "catdog",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

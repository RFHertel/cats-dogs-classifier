{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3504dabd",
   "metadata": {},
   "source": [
    "## Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa30f5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from transformers import pipeline\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e50a83",
   "metadata": {},
   "source": [
    "## Now use old cleaning techniques from tiny dataset on the whole dataset V3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c94df1c6",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 23\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# -----------------------------------------------------------------------------\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# SETUP: Load CLIP model\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# -----------------------------------------------------------------------------\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m classifier \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mzero-shot-image-classification\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mopenai/clip-vit-large-patch14-336\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\n\u001b[0;32m     27\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# -----------------------------------------------------------------------------\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# THRESHOLDS\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# -----------------------------------------------------------------------------\u001b[39;00m\n\u001b[0;32m     33\u001b[0m PHOTO_THRESHOLD \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.60\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\robie\\miniconda3\\envs\\catdog\\lib\\site-packages\\transformers\\pipelines\\__init__.py:836\u001b[0m, in \u001b[0;36mpipeline\u001b[1;34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, revision, use_fast, token, device, device_map, dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[0;32m    834\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    835\u001b[0m     model_classes \u001b[38;5;241m=\u001b[39m targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m--> 836\u001b[0m     model \u001b[38;5;241m=\u001b[39m load_model(\n\u001b[0;32m    837\u001b[0m         adapter_path \u001b[38;5;28;01mif\u001b[39;00m adapter_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m model,\n\u001b[0;32m    838\u001b[0m         model_classes\u001b[38;5;241m=\u001b[39mmodel_classes,\n\u001b[0;32m    839\u001b[0m         config\u001b[38;5;241m=\u001b[39mconfig,\n\u001b[0;32m    840\u001b[0m         task\u001b[38;5;241m=\u001b[39mtask,\n\u001b[0;32m    841\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs,\n\u001b[0;32m    842\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m    843\u001b[0m     )\n\u001b[0;32m    845\u001b[0m hub_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_commit_hash\n\u001b[0;32m    847\u001b[0m \u001b[38;5;66;03m# Check which preprocessing classes the pipeline uses\u001b[39;00m\n\u001b[0;32m    848\u001b[0m \u001b[38;5;66;03m# None values indicate optional classes that the pipeline can run without, we don't raise errors if loading fails\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\robie\\miniconda3\\envs\\catdog\\lib\\site-packages\\transformers\\pipelines\\base.py:232\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(model, config, model_classes, task, **model_kwargs)\u001b[0m\n\u001b[0;32m    229\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m model_kwargs\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m    231\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 232\u001b[0m     model \u001b[38;5;241m=\u001b[39m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(model, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;66;03m# Stop loading on the first successful load.\u001b[39;00m\n\u001b[0;32m    234\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\robie\\miniconda3\\envs\\catdog\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:372\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    370\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mconfig_class \u001b[38;5;241m==\u001b[39m config\u001b[38;5;241m.\u001b[39msub_configs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_config\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    371\u001b[0m         config \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget_text_config()\n\u001b[1;32m--> 372\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    373\u001b[0m         pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39mmodel_args, config\u001b[38;5;241m=\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    374\u001b[0m     )\n\u001b[0;32m    375\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    376\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    377\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    378\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\robie\\miniconda3\\envs\\catdog\\lib\\site-packages\\transformers\\modeling_utils.py:4109\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   4093\u001b[0m \u001b[38;5;66;03m# Finalize model weight initialization\u001b[39;00m\n\u001b[0;32m   4094\u001b[0m load_config \u001b[38;5;241m=\u001b[39m LoadStateDictConfig(\n\u001b[0;32m   4095\u001b[0m     pretrained_model_name_or_path\u001b[38;5;241m=\u001b[39mpretrained_model_name_or_path,\n\u001b[0;32m   4096\u001b[0m     ignore_mismatched_sizes\u001b[38;5;241m=\u001b[39mignore_mismatched_sizes,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4107\u001b[0m     download_kwargs\u001b[38;5;241m=\u001b[39mdownload_kwargs,\n\u001b[0;32m   4108\u001b[0m )\n\u001b[1;32m-> 4109\u001b[0m load_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint_files\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mload_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4110\u001b[0m load_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_finalize_load_state_dict(model, load_config, load_info)\n\u001b[0;32m   4111\u001b[0m model\u001b[38;5;241m.\u001b[39meval()  \u001b[38;5;66;03m# Set model in evaluation mode to deactivate Dropout modules by default\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\robie\\miniconda3\\envs\\catdog\\lib\\site-packages\\transformers\\modeling_utils.py:4231\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[1;34m(cls, model, state_dict, checkpoint_files, load_config)\u001b[0m\n\u001b[0;32m   4227\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   4228\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNeither a state dict nor checkpoint files were found.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   4230\u001b[0m missing_keys, unexpected_keys, mismatched_keys, disk_offload_index, conversion_errors \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m-> 4231\u001b[0m     \u001b[43mconvert_and_load_state_dict_in_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4232\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4233\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmerged_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4234\u001b[0m \u001b[43m        \u001b[49m\u001b[43mload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtp_plan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tp_plan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4236\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype_plan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype_plan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisk_offload_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4238\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4239\u001b[0m )\n\u001b[0;32m   4241\u001b[0m \u001b[38;5;66;03m# finally close all opened file pointers\u001b[39;00m\n\u001b[0;32m   4242\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m all_pointer:\n",
      "File \u001b[1;32mc:\\Users\\robie\\miniconda3\\envs\\catdog\\lib\\site-packages\\transformers\\core_model_loading.py:1211\u001b[0m, in \u001b[0;36mconvert_and_load_state_dict_in_model\u001b[1;34m(model, state_dict, load_config, tp_plan, dtype_plan, disk_offload_index)\u001b[0m\n\u001b[0;32m   1209\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1210\u001b[0m     total_entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(param_name_to_load)\n\u001b[1;32m-> 1211\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mlogging\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_entries\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLoading weights\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[0;32m   1212\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m first_param_name, mapping \u001b[38;5;129;01min\u001b[39;00m param_name_to_load\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m   1213\u001b[0m             pbar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\robie\\miniconda3\\envs\\catdog\\lib\\site-packages\\transformers\\utils\\logging.py:375\u001b[0m, in \u001b[0;36m_tqdm_cls.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    374\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _tqdm_active:\n\u001b[1;32m--> 375\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m tqdm_lib\u001b[38;5;241m.\u001b[39mtqdm(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    376\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    377\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m EmptyTqdm(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\robie\\miniconda3\\envs\\catdog\\lib\\site-packages\\tqdm\\notebook.py:238\u001b[0m, in \u001b[0;36mtqdm_notebook.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisplayed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m display_here \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelay \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 238\u001b[0m     \u001b[43mdisplay\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontainer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    239\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisplayed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    240\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisplay\n",
      "File \u001b[1;32mc:\\Users\\robie\\miniconda3\\envs\\catdog\\lib\\site-packages\\IPython\\core\\display_functions.py:305\u001b[0m, in \u001b[0;36mdisplay\u001b[1;34m(include, exclude, metadata, transient, display_id, raw, clear, *objs, **kwargs)\u001b[0m\n\u001b[0;32m    302\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m metadata:\n\u001b[0;32m    303\u001b[0m             \u001b[38;5;66;03m# kwarg-specified metadata gets precedence\u001b[39;00m\n\u001b[0;32m    304\u001b[0m             _merge(md_dict, metadata)\n\u001b[1;32m--> 305\u001b[0m         publish_display_data(data\u001b[38;5;241m=\u001b[39mformat_dict, metadata\u001b[38;5;241m=\u001b[39mmd_dict, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    306\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m display_id:\n\u001b[0;32m    307\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DisplayHandle(display_id)\n",
      "File \u001b[1;32mc:\\Users\\robie\\miniconda3\\envs\\catdog\\lib\\site-packages\\IPython\\core\\display_functions.py:93\u001b[0m, in \u001b[0;36mpublish_display_data\u001b[1;34m(data, metadata, source, transient, **kwargs)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transient:\n\u001b[0;32m     91\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransient\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m transient\n\u001b[1;32m---> 93\u001b[0m display_pub\u001b[38;5;241m.\u001b[39mpublish(\n\u001b[0;32m     94\u001b[0m     data\u001b[38;5;241m=\u001b[39mdata,\n\u001b[0;32m     95\u001b[0m     metadata\u001b[38;5;241m=\u001b[39mmetadata,\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m     97\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\robie\\miniconda3\\envs\\catdog\\lib\\site-packages\\ipykernel\\zmqshell.py:149\u001b[0m, in \u001b[0;36mZMQDisplayPublisher.publish\u001b[1;34m(self, data, metadata, transient, update, **kwargs)\u001b[0m\n\u001b[0;32m    145\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m outputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    146\u001b[0m         outputs\u001b[38;5;241m.\u001b[39msetdefault(exec_count, [])\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m    147\u001b[0m             HistoryOutput(output_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisplay_data\u001b[39m\u001b[38;5;124m\"\u001b[39m, bundle\u001b[38;5;241m=\u001b[39mdata)\n\u001b[0;32m    148\u001b[0m         )\n\u001b[1;32m--> 149\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flush_streams\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m metadata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    151\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[1;32mc:\\Users\\robie\\miniconda3\\envs\\catdog\\lib\\site-packages\\ipykernel\\zmqshell.py:93\u001b[0m, in \u001b[0;36mZMQDisplayPublisher._flush_streams\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_flush_streams\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m     92\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"flush IO Streams prior to display\"\"\"\u001b[39;00m\n\u001b[1;32m---> 93\u001b[0m     \u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstdout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflush\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     94\u001b[0m     sys\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mflush()\n",
      "File \u001b[1;32mc:\\Users\\robie\\miniconda3\\envs\\catdog\\lib\\site-packages\\ipykernel\\iostream.py:604\u001b[0m, in \u001b[0;36mOutStream.flush\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    602\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpub_thread\u001b[38;5;241m.\u001b[39mschedule(evt\u001b[38;5;241m.\u001b[39mset)\n\u001b[0;32m    603\u001b[0m     \u001b[38;5;66;03m# and give a timeout to avoid\u001b[39;00m\n\u001b[1;32m--> 604\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mevt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflush_timeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    605\u001b[0m         \u001b[38;5;66;03m# write directly to __stderr__ instead of warning because\u001b[39;00m\n\u001b[0;32m    606\u001b[0m         \u001b[38;5;66;03m# if this is happening sys.stderr may be the problem.\u001b[39;00m\n\u001b[0;32m    607\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIOStream.flush timed out\u001b[39m\u001b[38;5;124m\"\u001b[39m, file\u001b[38;5;241m=\u001b[39msys\u001b[38;5;241m.\u001b[39m__stderr__)\n\u001b[0;32m    608\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\robie\\miniconda3\\envs\\catdog\\lib\\threading.py:607\u001b[0m, in \u001b[0;36mEvent.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    605\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[0;32m    606\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[1;32m--> 607\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    608\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[1;32mc:\\Users\\robie\\miniconda3\\envs\\catdog\\lib\\threading.py:324\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    322\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    323\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 324\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    325\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    326\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# TEST 4: FULL PETIMAGES DATASET - CLIP FILTER WITH BLUR DETECTION\n",
    "# =============================================================================\n",
    "# \n",
    "# This processes the entire PetImages dataset and outputs:\n",
    "# 1. clip_exclude_list.txt - paths only (same format as exclude_list.txt)\n",
    "# 2. clip_exclude_details.txt - rejected images with scores and reasons\n",
    "# 3. clip_all_images.txt - ALL images with status, scores, reasons\n",
    "#\n",
    "# =============================================================================\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from transformers import pipeline\n",
    "import time\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# SETUP: Load CLIP model\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "classifier = pipeline(\n",
    "    \"zero-shot-image-classification\",\n",
    "    model=\"openai/clip-vit-large-patch14-336\",\n",
    "    device=0\n",
    ")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# THRESHOLDS\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "PHOTO_THRESHOLD = 0.60\n",
    "PHOTO_THRESHOLD_LENIENT = 0.40\n",
    "ANIMAL_THRESHOLD = 0.50\n",
    "ANIMAL_THRESHOLD_LENIENT = 0.15\n",
    "TEXT_THRESHOLD = 0.35\n",
    "BLUR_THRESHOLD = 5500\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# HELPER FUNCTIONS\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def get_blur_score(filepath):\n",
    "    \"\"\"Calculate blur score using Laplacian variance. Low = blurry, High = sharp.\"\"\"\n",
    "    img = cv2.imread(str(filepath), cv2.IMREAD_GRAYSCALE)\n",
    "    if img is None:\n",
    "        return 0\n",
    "    return cv2.Laplacian(img, cv2.CV_64F).var()\n",
    "\n",
    "def check_image_multi(filepath):\n",
    "    \"\"\"Run three independent CLIP classifications. Returns (photo_score, animal_score, text_score).\"\"\"\n",
    "    img = Image.open(filepath)\n",
    "    \n",
    "    result1 = classifier(img, candidate_labels=[\"camera photograph\", \"digital artwork\"])\n",
    "    photo_score = result1[0]['score'] if result1[0]['label'] == \"camera photograph\" else 1 - result1[0]['score']\n",
    "    \n",
    "    result2 = classifier(img, candidate_labels=[\"an animal\", \"not an animal\"])\n",
    "    animal_score = result2[0]['score'] if result2[0]['label'] == \"an animal\" else 1 - result2[0]['score']\n",
    "    \n",
    "    result3 = classifier(img, candidate_labels=[\"text and words\", \"no text\"])\n",
    "    text_score = result3[0]['score'] if result3[0]['label'] == \"text and words\" else 1 - result3[0]['score']\n",
    "    \n",
    "    return photo_score, animal_score, text_score\n",
    "\n",
    "def should_keep_v3(photo_score, animal_score, text_score, blur_score):\n",
    "    \"\"\"Determine if image should be kept based on CLIP scores and blur.\"\"\"\n",
    "    is_blurry = blur_score < BLUR_THRESHOLD\n",
    "    \n",
    "    if photo_score >= PHOTO_THRESHOLD and animal_score >= ANIMAL_THRESHOLD:\n",
    "        return True, \"real photo with animal\"\n",
    "    \n",
    "    if is_blurry:\n",
    "        if photo_score >= PHOTO_THRESHOLD_LENIENT:\n",
    "            if animal_score >= ANIMAL_THRESHOLD_LENIENT:\n",
    "                return True, \"blurry photo with animal (lenient)\"\n",
    "            if animal_score < ANIMAL_THRESHOLD_LENIENT:\n",
    "                if text_score >= TEXT_THRESHOLD:\n",
    "                    return False, \"blurry photo, no animal, has text\"\n",
    "                return False, \"blurry photo but no animal\"\n",
    "    \n",
    "    if photo_score < PHOTO_THRESHOLD_LENIENT:\n",
    "        return False, \"not a real photo\"\n",
    "    \n",
    "    if animal_score < ANIMAL_THRESHOLD:\n",
    "        if text_score >= TEXT_THRESHOLD:\n",
    "            return False, \"real photo, no animal, has text\"\n",
    "        return False, \"real photo but no animal\"\n",
    "    \n",
    "    return False, \"did not meet criteria\"\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# PATHS\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "data_path = Path(r\"C:\\AWrk\\cats_dogs_project\\data\\PetImages\")\n",
    "output_dir = Path(r\"C:\\AWrk\\cats_dogs_project\\outputs\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "exclude_list_path = output_dir / \"clip_exclude_list.txt\"\n",
    "exclude_details_path = output_dir / \"clip_exclude_details.txt\"\n",
    "all_images_path = output_dir / \"clip_all_images.txt\"\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# LOAD DATASET\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "cat_files = list((data_path / \"Cat\").glob(\"*.jpg\"))\n",
    "dog_files = list((data_path / \"Dog\").glob(\"*.jpg\"))\n",
    "all_files = cat_files + dog_files\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(\"TEST 4: FULL PETIMAGES DATASET\")\n",
    "print(\"=\" * 100)\n",
    "print(f\"Total images to process: {len(all_files)}\")\n",
    "print(f\"  Cats: {len(cat_files)}\")\n",
    "print(f\"  Dogs: {len(dog_files)}\")\n",
    "print()\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# PROCESS ALL IMAGES\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "kept = []\n",
    "rejected = []\n",
    "errors = []\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "print(\"Processing images...\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "with open(exclude_details_path, \"w\") as details_file, \\\n",
    "     open(all_images_path, \"w\") as all_file:\n",
    "    \n",
    "    # Write headers\n",
    "    details_file.write(\"# CLIP Filter Results - Rejected Images Only\\n\")\n",
    "    details_file.write(\"# path\\tphoto\\tanimal\\ttext\\tblur\\treason\\n\")\n",
    "    details_file.write(\"-\" * 100 + \"\\n\")\n",
    "    \n",
    "    all_file.write(\"# CLIP Filter Results - All Images\\n\")\n",
    "    all_file.write(\"# path\\tstatus\\tphoto\\tanimal\\ttext\\tblur\\treason\\n\")\n",
    "    all_file.write(\"-\" * 120 + \"\\n\")\n",
    "    \n",
    "    for i, f in enumerate(all_files):\n",
    "        # Relative path format matching existing exclude_list.txt\n",
    "        rel_path = f\"..\\\\data\\\\PetImages\\\\{f.parent.name}\\\\{f.name}\"\n",
    "        \n",
    "        try:\n",
    "            photo, animal, text = check_image_multi(f)\n",
    "            blur = get_blur_score(f)\n",
    "            keep, reason = should_keep_v3(photo, animal, text, blur)\n",
    "            \n",
    "            status = \"KEEP\" if keep else \"REJECT\"\n",
    "            \n",
    "            # Write to all images file\n",
    "            all_file.write(f\"{rel_path}\\t{status}\\t{photo:.2f}\\t{animal:.2f}\\t{text:.2f}\\t{blur:.1f}\\t{reason}\\n\")\n",
    "            \n",
    "            if keep:\n",
    "                kept.append((rel_path, photo, animal, text, blur, reason))\n",
    "            else:\n",
    "                rejected.append((rel_path, photo, animal, text, blur, reason))\n",
    "                details_file.write(f\"{rel_path}\\t{photo:.2f}\\t{animal:.2f}\\t{text:.2f}\\t{blur:.1f}\\t{reason}\\n\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            errors.append((rel_path, str(e)))\n",
    "            rejected.append((rel_path, 0, 0, 0, 0, f\"ERROR: {e}\"))\n",
    "            details_file.write(f\"{rel_path}\\tERROR\\t\\t\\t\\t{e}\\n\")\n",
    "            all_file.write(f\"{rel_path}\\tERROR\\t\\t\\t\\t\\t{e}\\n\")\n",
    "        \n",
    "        # Progress update every 500 images\n",
    "        if (i + 1) % 500 == 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            rate = (i + 1) / elapsed\n",
    "            remaining = (len(all_files) - i - 1) / rate\n",
    "            print(f\"  {i + 1}/{len(all_files)} ({(i+1)/len(all_files)*100:.1f}%) - \"\n",
    "                  f\"Rejected: {len(rejected)} - \"\n",
    "                  f\"ETA: {remaining/60:.1f} min\")\n",
    "\n",
    "# Write simple exclude list (just paths)\n",
    "with open(exclude_list_path, \"w\") as f:\n",
    "    for rel_path, photo, animal, text, blur, reason in rejected:\n",
    "        f.write(rel_path + \"\\n\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# FINAL SUMMARY\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "print()\n",
    "print(\"=\" * 100)\n",
    "print(\"COMPLETE\")\n",
    "print(\"=\" * 100)\n",
    "print(f\"Time elapsed: {elapsed/60:.1f} minutes\")\n",
    "print(f\"Total processed: {len(all_files)}\")\n",
    "print(f\"KEPT: {len(kept)}\")\n",
    "print(f\"REJECTED: {len(rejected)}\")\n",
    "print(f\"ERRORS: {len(errors)}\")\n",
    "\n",
    "# Breakdown by class\n",
    "rejected_cats = sum(1 for r in rejected if \"\\\\Cat\\\\\" in r[0])\n",
    "rejected_dogs = sum(1 for r in rejected if \"\\\\Dog\\\\\" in r[0])\n",
    "print()\n",
    "print(f\"Rejected Cats: {rejected_cats}\")\n",
    "print(f\"Rejected Dogs: {rejected_dogs}\")\n",
    "\n",
    "# Breakdown by reason\n",
    "print()\n",
    "print(\"Rejection reasons:\")\n",
    "reason_counts = {}\n",
    "for _, _, _, _, _, reason in rejected:\n",
    "    reason_counts[reason] = reason_counts.get(reason, 0) + 1\n",
    "for reason, count in sorted(reason_counts.items(), key=lambda x: -x[1]):\n",
    "    print(f\"  {count:5d}  {reason}\")\n",
    "\n",
    "# Output files\n",
    "print()\n",
    "print(\"Output files:\")\n",
    "print(f\"  1. {exclude_list_path}\")\n",
    "print(f\"     Paths only (same format as exclude_list.txt)\")\n",
    "print()\n",
    "print(f\"  2. {exclude_details_path}\")\n",
    "print(f\"     Rejected images with scores and reasons\")\n",
    "print()\n",
    "print(f\"  3. {all_images_path}\")\n",
    "print(f\"     ALL images with status, scores, and reasons\")\n",
    "\n",
    "# Show first 20 rejected for quick review\n",
    "print()\n",
    "print(\"First 20 rejected images:\")\n",
    "print(\"-\" * 100)\n",
    "print(f\"{'path':<45} {'photo':>5} {'animal':>6} {'text':>5} {'blur':>8}  reason\")\n",
    "print(\"-\" * 100)\n",
    "for rel_path, photo, animal, text, blur, reason in rejected[:20]:\n",
    "    print(f\"{rel_path:<45} {photo:>5.2f} {animal:>6.2f} {text:>5.2f} {blur:>8.1f}  {reason}\")\n",
    "\n",
    "if errors:\n",
    "    print()\n",
    "    print(f\"Errors ({len(errors)}):\")\n",
    "    for path, err in errors[:10]:\n",
    "        print(f\"  {path}: {err}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b78461",
   "metadata": {},
   "source": [
    "## Problems with V3\n",
    "\n",
    "The code needed to be updated as we were identifying images that were cat and dog images as rejected. It was however working and truly rejecting bad images which was good. There were just a few to many false positives. The false positive were mostly on blurry images that had high animal threshold and on multiple animals in the image.\n",
    "\n",
    "Fixes:\n",
    "\n",
    "1. High Animal Override Now Requires Very Low Blur\n",
    "New: If animal â‰¥ 0.80 AND blur < 500, then keep it\n",
    "The critical insight is that drawings of animals also score high on animal detection but are NOT blurry. Real blurry photos have blur < 500, while drawings/edited images have blur > 1500. The old version was incorrectly keeping sharp drawings because they had high animal scores.\n",
    "\n",
    "2. If a photo in very clearly real (clip tells us) and the animal is low try a multiple animals check - clip struggles with multiples and needs specific prompting for it so the new rule should be: photo >= 0.90 AND animal < 0.50 then run multiple >= 0.30 OR single >= 0.30 to keep the images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef15cd4",
   "metadata": {},
   "source": [
    "# V3+ new rescue rules:\n",
    "\n",
    "  Rescue 1: blur < 500 AND animal >= 0.80 AND photo >= 0.25                  \n",
    "              KEEP \"RESCUED: very blurry with high animal confidence\"      \n",
    "                                                                             \n",
    "            Why: Real blurry photos have blur < 500                          \n",
    "                 Drawings have sharp edges (blur > 1500)                     \n",
    "            Catches: yawning cat, unusual poses, solid backgrounds           \n",
    "                                                                             \n",
    "  Rescue 2: photo >= 0.90 AND animal < 0.50                                  \n",
    "            Run secondary CLIP check for multiple animals                \n",
    "              multiple >= 0.30 OR single >= 0.30                           \n",
    "              KEEP \"RESCUED: multiple animals detected\"                \n",
    "                                                                             \n",
    "            Why: \"an animal\" prompt fails with multiple cats/dogs            \n",
    "            Catches: group photos, cats in cages, kittens held by humans     \n",
    "                                                                             \n",
    "  No rescue applied:                                                         \n",
    "              REJECT (return V3's original rejection reason) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05a0d103",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee274de2a27c4d82ac98d49d3d7e878e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/590 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CLIPModel LOAD REPORT from: openai/clip-vit-large-patch14-336\n",
      "Key                                  | Status     |  | \n",
      "-------------------------------------+------------+--+-\n",
      "vision_model.embeddings.position_ids | UNEXPECTED |  | \n",
      "text_model.embeddings.position_ids   | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "The image processor of type `CLIPImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================================================================================================\n",
      "FULL PETIMAGES DATASET - V3+ ALGORITHM\n",
      "==============================================================================================================\n",
      "Total images to process: 25000\n",
      "  Cats: 12500\n",
      "  Dogs: 12500\n",
      "\n",
      "V3+ Design: V3 decisions preserved, rescue rules can only SAVE rejected images\n",
      "\n",
      "Rescue Rules:\n",
      "  1. Very blurry + high animal: blur < 500, animal >= 0.8, photo >= 0.25\n",
      "  2. Multiple animals fallback: photo >= 0.9, animal < 0.5, then CLIP check\n",
      "\n",
      "Processing images...\n",
      "--------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  500/25000 (2.0%) - Kept: 498 (rescued: 0) - Rejected: 2 - ETA: 183.2 min\n",
      "  1000/25000 (4.0%) - Kept: 997 (rescued: 1) - Rejected: 3 - ETA: 179.3 min\n",
      "  1500/25000 (6.0%) - Kept: 1496 (rescued: 3) - Rejected: 4 - ETA: 176.9 min\n",
      "  2000/25000 (8.0%) - Kept: 1995 (rescued: 4) - Rejected: 5 - ETA: 173.5 min\n",
      "  2500/25000 (10.0%) - Kept: 2494 (rescued: 4) - Rejected: 6 - ETA: 170.1 min\n",
      "  3000/25000 (12.0%) - Kept: 2994 (rescued: 5) - Rejected: 6 - ETA: 166.5 min\n",
      "  3500/25000 (14.0%) - Kept: 3492 (rescued: 5) - Rejected: 8 - ETA: 162.9 min\n",
      "  4000/25000 (16.0%) - Kept: 3990 (rescued: 5) - Rejected: 10 - ETA: 159.1 min\n",
      "  4500/25000 (18.0%) - Kept: 4489 (rescued: 5) - Rejected: 11 - ETA: 155.3 min\n",
      "  5000/25000 (20.0%) - Kept: 4987 (rescued: 7) - Rejected: 13 - ETA: 151.6 min\n",
      "  5500/25000 (22.0%) - Kept: 5485 (rescued: 10) - Rejected: 15 - ETA: 147.9 min\n",
      "  6000/25000 (24.0%) - Kept: 5985 (rescued: 11) - Rejected: 15 - ETA: 144.1 min\n",
      "  6500/25000 (26.0%) - Kept: 6483 (rescued: 11) - Rejected: 17 - ETA: 140.3 min\n",
      "  7000/25000 (28.0%) - Kept: 6980 (rescued: 12) - Rejected: 20 - ETA: 136.5 min\n",
      "  7500/25000 (30.0%) - Kept: 7478 (rescued: 12) - Rejected: 22 - ETA: 132.7 min\n",
      "  8000/25000 (32.0%) - Kept: 7975 (rescued: 13) - Rejected: 25 - ETA: 128.9 min\n",
      "  8500/25000 (34.0%) - Kept: 8474 (rescued: 13) - Rejected: 26 - ETA: 125.1 min\n",
      "  9000/25000 (36.0%) - Kept: 8972 (rescued: 13) - Rejected: 28 - ETA: 121.3 min\n",
      "  9500/25000 (38.0%) - Kept: 9471 (rescued: 14) - Rejected: 29 - ETA: 117.5 min\n",
      "  10000/25000 (40.0%) - Kept: 9970 (rescued: 14) - Rejected: 30 - ETA: 130.9 min\n",
      "  10500/25000 (42.0%) - Kept: 10469 (rescued: 14) - Rejected: 31 - ETA: 125.9 min\n",
      "  11000/25000 (44.0%) - Kept: 10963 (rescued: 15) - Rejected: 37 - ETA: 121.0 min\n",
      "  11500/25000 (46.0%) - Kept: 11460 (rescued: 16) - Rejected: 40 - ETA: 116.1 min\n",
      "  12000/25000 (48.0%) - Kept: 11957 (rescued: 16) - Rejected: 43 - ETA: 111.4 min\n",
      "  12500/25000 (50.0%) - Kept: 12456 (rescued: 16) - Rejected: 44 - ETA: 106.7 min\n",
      "  13000/25000 (52.0%) - Kept: 12951 (rescued: 16) - Rejected: 49 - ETA: 102.1 min\n",
      "  13500/25000 (54.0%) - Kept: 13449 (rescued: 16) - Rejected: 51 - ETA: 97.5 min\n",
      "  14000/25000 (56.0%) - Kept: 13944 (rescued: 16) - Rejected: 56 - ETA: 92.9 min\n",
      "  14500/25000 (58.0%) - Kept: 14441 (rescued: 16) - Rejected: 59 - ETA: 88.5 min\n",
      "  15000/25000 (60.0%) - Kept: 14941 (rescued: 16) - Rejected: 59 - ETA: 84.1 min\n",
      "  15500/25000 (62.0%) - Kept: 15438 (rescued: 16) - Rejected: 62 - ETA: 79.6 min\n",
      "  16000/25000 (64.0%) - Kept: 15936 (rescued: 16) - Rejected: 64 - ETA: 75.3 min\n",
      "  16500/25000 (66.0%) - Kept: 16435 (rescued: 17) - Rejected: 65 - ETA: 70.9 min\n",
      "  17000/25000 (68.0%) - Kept: 16935 (rescued: 19) - Rejected: 65 - ETA: 66.6 min\n",
      "  17500/25000 (70.0%) - Kept: 17434 (rescued: 20) - Rejected: 66 - ETA: 62.4 min\n",
      "  18000/25000 (72.0%) - Kept: 17934 (rescued: 21) - Rejected: 66 - ETA: 58.1 min\n",
      "  18500/25000 (74.0%) - Kept: 18433 (rescued: 21) - Rejected: 67 - ETA: 53.9 min\n",
      "  19000/25000 (76.0%) - Kept: 18931 (rescued: 22) - Rejected: 69 - ETA: 49.6 min\n",
      "  19500/25000 (78.0%) - Kept: 19430 (rescued: 24) - Rejected: 70 - ETA: 45.4 min\n",
      "  20000/25000 (80.0%) - Kept: 19928 (rescued: 24) - Rejected: 72 - ETA: 41.2 min\n",
      "  20500/25000 (82.0%) - Kept: 20428 (rescued: 25) - Rejected: 72 - ETA: 37.1 min\n",
      "  21000/25000 (84.0%) - Kept: 20927 (rescued: 25) - Rejected: 73 - ETA: 32.9 min\n",
      "  21500/25000 (86.0%) - Kept: 21426 (rescued: 27) - Rejected: 74 - ETA: 28.7 min\n",
      "  22000/25000 (88.0%) - Kept: 21924 (rescued: 27) - Rejected: 76 - ETA: 24.6 min\n",
      "  22500/25000 (90.0%) - Kept: 22420 (rescued: 27) - Rejected: 80 - ETA: 20.5 min\n",
      "  23000/25000 (92.0%) - Kept: 22920 (rescued: 28) - Rejected: 80 - ETA: 16.4 min\n",
      "  23500/25000 (94.0%) - Kept: 23420 (rescued: 29) - Rejected: 80 - ETA: 12.3 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\robie\\miniconda3\\envs\\catdog\\lib\\site-packages\\PIL\\TiffImagePlugin.py:949: UserWarning: Truncated File Read\n",
      "  warnings.warn(str(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  24000/25000 (96.0%) - Kept: 23917 (rescued: 29) - Rejected: 83 - ETA: 8.2 min\n",
      "  24500/25000 (98.0%) - Kept: 24414 (rescued: 30) - Rejected: 86 - ETA: 4.1 min\n",
      "  25000/25000 (100.0%) - Kept: 24914 (rescued: 31) - Rejected: 86 - ETA: 0.0 min\n",
      "\n",
      "==============================================================================================================\n",
      "COMPLETE - V3+ ALGORITHM\n",
      "==============================================================================================================\n",
      "Time elapsed: 203.8 minutes\n",
      "Total processed: 25000\n",
      "\n",
      "KEPT:     24914\n",
      "  - By V3 rules:    24883\n",
      "  - By rescue rules: 31\n",
      "REJECTED: 86\n",
      "ERRORS:   2\n",
      "\n",
      "By class:\n",
      "  Rejected Cats: 44\n",
      "  Rejected Dogs: 42\n",
      "  Rescued Cats:  16\n",
      "  Rescued Dogs:  15\n",
      "\n",
      "Rejection reasons:\n",
      "     43  not a real photo\n",
      "     22  blurry photo, no animal, has text\n",
      "      7  real photo, no animal, has text\n",
      "      6  blurry photo but no animal\n",
      "      3  did not meet criteria\n",
      "      3  real photo but no animal\n",
      "      1  ERROR: cannot identify image file 'C:\\\\AWrk\\\\cats_dogs_project\\\\data\\\\PetImages\\\\Cat\\\\666.jpg'\n",
      "      1  ERROR: cannot identify image file 'C:\\\\AWrk\\\\cats_dogs_project\\\\data\\\\PetImages\\\\Dog\\\\11702.jpg'\n",
      "\n",
      "Rescue reasons:\n",
      "     23  RESCUED: multiple animals detected\n",
      "      8  RESCUED: very blurry with high animal\n",
      "\n",
      "Output files:\n",
      "  1. C:\\AWrk\\cats_dogs_project\\outputs\\clip_exclude_list_v3plus.txt\n",
      "     Paths only (for loading into training)\n",
      "\n",
      "  2. C:\\AWrk\\cats_dogs_project\\outputs\\clip_exclude_details_v3plus.txt\n",
      "     Rejected images with scores and reasons\n",
      "\n",
      "  3. C:\\AWrk\\cats_dogs_project\\outputs\\clip_all_images_v3plus.txt\n",
      "     ALL images with status, scores, and reasons\n",
      "\n",
      "  4. C:\\AWrk\\cats_dogs_project\\outputs\\clip_rescued_images_v3plus.txt\n",
      "     Images saved by rescue rules (would have been rejected by V3)\n",
      "\n",
      "RESCUED IMAGES (31 total) - First 30:\n",
      "----------------------------------------------------------------------------------------------------------------------------------\n",
      "path                                               photo animal  text     blur  reason                                   rescue_details\n",
      "----------------------------------------------------------------------------------------------------------------------------------\n",
      "Cat\\10607.jpg                                       0.30   0.97  0.18    103.3  RESCUED: very blurry with high animal    blur=103, animal=0.97\n",
      "Cat\\11197.jpg                                       0.98   0.13  0.59    172.6  RESCUED: multiple animals detected       multi=0.38, single=0.02\n",
      "Cat\\11345.jpg                                       1.00   0.13  0.81    398.0  RESCUED: multiple animals detected       multi=0.49, single=0.06\n",
      "Cat\\11786.jpg                                       0.99   0.15  0.63    302.9  RESCUED: multiple animals detected       multi=0.62, single=0.05\n",
      "Cat\\1429.jpg                                        0.95   0.14  0.76    162.2  RESCUED: multiple animals detected       multi=0.70, single=0.02\n",
      "Cat\\3182.jpg                                        0.98   0.13  0.58    661.6  RESCUED: multiple animals detected       multi=0.41, single=0.08\n",
      "Cat\\3220.jpg                                        0.90   0.14  0.53    417.4  RESCUED: multiple animals detected       multi=0.52, single=0.05\n",
      "Cat\\3469.jpg                                        0.96   0.25  0.60   9483.5  RESCUED: multiple animals detected       multi=0.92, single=0.02\n",
      "Cat\\3500.jpg                                        0.37   0.83  0.53    159.5  RESCUED: very blurry with high animal    blur=160, animal=0.83\n",
      "Cat\\3504.jpg                                        0.95   0.15  0.15     40.2  RESCUED: multiple animals detected       multi=0.30, single=0.12\n",
      "Cat\\3910.jpg                                        0.98   0.13  0.59    172.6  RESCUED: multiple animals detected       multi=0.38, single=0.02\n",
      "Cat\\4812.jpg                                        0.99   0.13  0.82    391.1  RESCUED: multiple animals detected       multi=0.38, single=0.08\n",
      "Cat\\5862.jpg                                        0.31   0.99  0.92    388.6  RESCUED: very blurry with high animal    blur=389, animal=0.99\n",
      "Cat\\6911.jpg                                        0.99   0.32  0.89   7158.6  RESCUED: multiple animals detected       multi=0.65, single=0.03\n",
      "Cat\\8521.jpg                                        0.37   0.84  0.47     72.5  RESCUED: very blurry with high animal    blur=73, animal=0.84\n",
      "Cat\\8937.jpg                                        0.34   0.90  0.17    364.4  RESCUED: very blurry with high animal    blur=364, animal=0.90\n",
      "Dog\\1920.jpg                                        0.96   0.43  0.77  20464.8  RESCUED: multiple animals detected       multi=0.89, single=0.06\n",
      "Dog\\2517.jpg                                        0.99   0.32  0.90   5647.5  RESCUED: multiple animals detected       multi=0.71, single=0.05\n",
      "Dog\\2732.jpg                                        0.99   0.44  0.93  12381.9  RESCUED: multiple animals detected       multi=0.36, single=0.35\n",
      "Dog\\301.jpg                                         0.98   0.46  0.94   6243.3  RESCUED: multiple animals detected       multi=0.49, single=0.10\n",
      "Dog\\3393.jpg                                        0.99   0.13  0.86   2768.5  RESCUED: multiple animals detected       multi=0.50, single=0.01\n",
      "Dog\\441.jpg                                         0.33   0.88  0.92    321.4  RESCUED: very blurry with high animal    blur=321, animal=0.88\n",
      "Dog\\4894.jpg                                        0.98   0.34  0.82   8778.0  RESCUED: multiple animals detected       multi=0.61, single=0.05\n",
      "Dog\\4984.jpg                                        0.36   0.98  0.56     55.1  RESCUED: very blurry with high animal    blur=55, animal=0.98\n",
      "Dog\\564.jpg                                         0.99   0.49  0.55   6311.9  RESCUED: multiple animals detected       multi=0.05, single=0.38\n",
      "Dog\\6509.jpg                                        0.97   0.36  0.85   6913.5  RESCUED: multiple animals detected       multi=0.52, single=0.03\n",
      "Dog\\6685.jpg                                        0.97   0.48  0.42  12673.2  RESCUED: multiple animals detected       multi=0.52, single=0.16\n",
      "Dog\\8119.jpg                                        0.99   0.12  0.55     97.9  RESCUED: multiple animals detected       multi=0.63, single=0.04\n",
      "Dog\\8508.jpg                                        0.99   0.44  0.87  13175.8  RESCUED: multiple animals detected       multi=0.34, single=0.04\n",
      "Dog\\9371.jpg                                        0.38   0.98  0.09    307.2  RESCUED: very blurry with high animal    blur=307, animal=0.98\n",
      "... and 1 more (see C:\\AWrk\\cats_dogs_project\\outputs\\clip_rescued_images_v3plus.txt)\n",
      "\n",
      "First 20 rejected images:\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "path                                               photo animal  text     blur  reason\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Cat\\10029.jpg                                       0.89   0.14  0.27     84.6  blurry photo but no animal\n",
      "Cat\\10035.jpg                                       0.34   0.95  0.14   1917.9  not a real photo\n",
      "Cat\\10827.jpg                                       0.37   0.75  0.19    138.2  not a real photo\n",
      "Cat\\11282.jpg                                       0.99   0.15  0.54    205.1  blurry photo, no animal, has text\n",
      "Cat\\11484.jpg                                       0.04   0.91  0.56   3500.6  not a real photo\n",
      "Cat\\1210.jpg                                        1.00   0.07  0.15      6.3  blurry photo but no animal\n",
      "Cat\\1600.jpg                                        0.38   0.86  0.46   2017.1  not a real photo\n",
      "Cat\\1885.jpg                                        0.97   0.46  0.59   8419.9  real photo, no animal, has text\n",
      "Cat\\2013.jpg                                        0.51   0.91  0.68  11694.5  did not meet criteria\n",
      "Cat\\2337.jpg                                        1.00   0.14  0.17    171.4  blurry photo but no animal\n",
      "Cat\\2663.jpg                                        0.06   0.80  0.66   7415.9  not a real photo\n",
      "Cat\\2939.jpg                                        0.07   0.82  0.28  18004.4  not a real photo\n",
      "Cat\\3016.jpg                                        0.04   0.45  0.02   3282.0  not a real photo\n",
      "Cat\\354.jpg                                         0.20   0.89  0.04    598.7  not a real photo\n",
      "Cat\\3632.jpg                                        1.00   0.14  0.75    466.3  blurry photo, no animal, has text\n",
      "Cat\\4314.jpg                                        0.27   0.52  0.17    109.4  not a real photo\n",
      "Cat\\4418.jpg                                        0.99   0.15  0.76    732.2  blurry photo, no animal, has text\n",
      "Cat\\4709.jpg                                        0.97   0.10  0.41     96.7  blurry photo, no animal, has text\n",
      "Cat\\4833.jpg                                        0.29   0.63  0.16  16381.2  not a real photo\n",
      "Cat\\5003.jpg                                        0.06   0.81  0.28    534.8  not a real photo\n",
      "\n",
      "Errors (2):\n",
      "  ..\\data\\PetImages\\Cat\\666.jpg: cannot identify image file 'C:\\\\AWrk\\\\cats_dogs_project\\\\data\\\\PetImages\\\\Cat\\\\666.jpg'\n",
      "  ..\\data\\PetImages\\Dog\\11702.jpg: cannot identify image file 'C:\\\\AWrk\\\\cats_dogs_project\\\\data\\\\PetImages\\\\Dog\\\\11702.jpg'\n",
      "\n",
      "==============================================================================================================\n",
      "V3+ ALGORITHM SUMMARY\n",
      "==============================================================================================================\n",
      "Design principle: If V3 keeps it â†’ V3+ keeps it. Always.\n",
      "Rescue rules can ONLY save images that V3 would reject.\n",
      "\n",
      "Rescue Rule 1: Very blurry + high animal\n",
      "  blur < 500, animal >= 0.8, photo >= 0.25\n",
      "  Catches: yawning cats, unusual poses, solid backgrounds\n",
      "\n",
      "Rescue Rule 2: Multiple animals fallback\n",
      "  photo >= 0.9, animal < 0.5\n",
      "  Secondary CLIP check, threshold >= 0.3\n",
      "  Catches: group photos, cats in cages, multiple kittens\n",
      "==============================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# TEST 4: FULL PETIMAGES DATASET - CLIP FILTER V3+ (V3 with Rescue Rules)\n",
    "# =============================================================================\n",
    "# \n",
    "# DESIGN PRINCIPLE:\n",
    "# If V3 keeps it â†’ V3+ keeps it. Always.\n",
    "# Rescue rules can ONLY save images that V3 would reject.\n",
    "#\n",
    "# RESCUE RULES:\n",
    "# 1. Very blurry + high animal: blur < 500, animal >= 0.80, photo >= 0.25\n",
    "# 2. Multiple animals fallback: photo >= 0.90, animal < 0.50 â†’ secondary CLIP check\n",
    "#\n",
    "# OUTPUTS:\n",
    "# 1. clip_exclude_list_v3plus.txt - paths only (for loading into training)\n",
    "# 2. clip_exclude_details_v3plus.txt - rejected images with scores and reasons\n",
    "# 3. clip_all_images_v3plus.txt - ALL images with status, scores, reasons\n",
    "# 4. clip_rescued_images_v3plus.txt - images saved by rescue rules\n",
    "#\n",
    "# =============================================================================\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from transformers import pipeline\n",
    "import time\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# SETUP: Load CLIP model\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "classifier = pipeline(\n",
    "    \"zero-shot-image-classification\",\n",
    "    model=\"openai/clip-vit-large-patch14-336\",\n",
    "    device=0\n",
    ")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# V3 THRESHOLDS (unchanged - proven baseline)\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "PHOTO_THRESHOLD = 0.60\n",
    "PHOTO_THRESHOLD_LENIENT = 0.40\n",
    "ANIMAL_THRESHOLD = 0.50\n",
    "ANIMAL_THRESHOLD_LENIENT = 0.15\n",
    "TEXT_THRESHOLD = 0.35\n",
    "BLUR_THRESHOLD = 5500\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# RESCUE RULE THRESHOLDS (V3+ additions)\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Rescue Rule 1: Very blurry + high animal\n",
    "RESCUE1_BLUR_MAX = 500          # Must be VERY blurry (real photos only)\n",
    "RESCUE1_ANIMAL_MIN = 0.80       # Must have high animal confidence\n",
    "RESCUE1_PHOTO_MIN = 0.25        # Must have SOME photo signal\n",
    "\n",
    "# Rescue Rule 2: Multiple animals fallback\n",
    "RESCUE2_PHOTO_MIN = 0.90        # Only try when photo is high\n",
    "RESCUE2_ANIMAL_MAX = 0.50       # Only when standard animal check failed\n",
    "RESCUE2_MULTIPLE_MIN = 0.30     # Threshold for multiple animals detection\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# HELPER FUNCTIONS\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def get_blur_score(filepath):\n",
    "    \"\"\"Calculate blur score using Laplacian variance. Low = blurry, High = sharp.\"\"\"\n",
    "    img = cv2.imread(str(filepath), cv2.IMREAD_GRAYSCALE)\n",
    "    if img is None:\n",
    "        return 0\n",
    "    return cv2.Laplacian(img, cv2.CV_64F).var()\n",
    "\n",
    "\n",
    "def check_image_multi(filepath):\n",
    "    \"\"\"Run three independent CLIP classifications. Returns (photo_score, animal_score, text_score).\"\"\"\n",
    "    img = Image.open(filepath)\n",
    "    \n",
    "    result1 = classifier(img, candidate_labels=[\"camera photograph\", \"digital artwork\"])\n",
    "    photo_score = result1[0]['score'] if result1[0]['label'] == \"camera photograph\" else 1 - result1[0]['score']\n",
    "    \n",
    "    result2 = classifier(img, candidate_labels=[\"an animal\", \"not an animal\"])\n",
    "    animal_score = result2[0]['score'] if result2[0]['label'] == \"an animal\" else 1 - result2[0]['score']\n",
    "    \n",
    "    result3 = classifier(img, candidate_labels=[\"text and words\", \"no text\"])\n",
    "    text_score = result3[0]['score'] if result3[0]['label'] == \"text and words\" else 1 - result3[0]['score']\n",
    "    \n",
    "    return photo_score, animal_score, text_score\n",
    "\n",
    "\n",
    "def check_multiple_animals(filepath):\n",
    "    \"\"\"\n",
    "    Secondary CLIP check for multiple animals.\n",
    "    Use when standard \"an animal\" check fails but image looks like a real photo.\n",
    "    Returns (multiple_score, single_score).\n",
    "    \"\"\"\n",
    "    img = Image.open(filepath)\n",
    "    result = classifier(img, candidate_labels=[\"multiple animals\", \"one animal\", \"no animals\"])\n",
    "    \n",
    "    scores = {r['label']: r['score'] for r in result}\n",
    "    multiple_score = scores.get(\"multiple animals\", 0)\n",
    "    single_score = scores.get(\"one animal\", 0)\n",
    "    \n",
    "    return multiple_score, single_score\n",
    "\n",
    "\n",
    "def should_keep_v3(photo_score, animal_score, text_score, blur_score):\n",
    "    \"\"\"Original V3 decision logic. Returns (keep, reason).\"\"\"\n",
    "    is_blurry = blur_score < BLUR_THRESHOLD\n",
    "    \n",
    "    if photo_score >= PHOTO_THRESHOLD and animal_score >= ANIMAL_THRESHOLD:\n",
    "        return True, \"real photo with animal\"\n",
    "    \n",
    "    if is_blurry:\n",
    "        if photo_score >= PHOTO_THRESHOLD_LENIENT:\n",
    "            if animal_score >= ANIMAL_THRESHOLD_LENIENT:\n",
    "                return True, \"blurry photo with animal (lenient)\"\n",
    "            if animal_score < ANIMAL_THRESHOLD_LENIENT:\n",
    "                if text_score >= TEXT_THRESHOLD:\n",
    "                    return False, \"blurry photo, no animal, has text\"\n",
    "                return False, \"blurry photo but no animal\"\n",
    "    \n",
    "    if photo_score < PHOTO_THRESHOLD_LENIENT:\n",
    "        return False, \"not a real photo\"\n",
    "    \n",
    "    if animal_score < ANIMAL_THRESHOLD:\n",
    "        if text_score >= TEXT_THRESHOLD:\n",
    "            return False, \"real photo, no animal, has text\"\n",
    "        return False, \"real photo but no animal\"\n",
    "    \n",
    "    return False, \"did not meet criteria\"\n",
    "\n",
    "\n",
    "def should_keep_v3plus(photo_score, animal_score, text_score, blur_score, filepath=None):\n",
    "    \"\"\"\n",
    "    V3+ decision logic: V3 baseline + rescue rules.\n",
    "    \n",
    "    Returns (keep, reason, rescued, rescue_details)\n",
    "        keep: bool\n",
    "        reason: str\n",
    "        rescued: bool - True if a rescue rule saved this image\n",
    "        rescue_details: str - details about rescue (e.g., multiple animal scores)\n",
    "    \"\"\"\n",
    "    # Step 1: Run V3 first\n",
    "    v3_keep, v3_reason = should_keep_v3(photo_score, animal_score, text_score, blur_score)\n",
    "    \n",
    "    if v3_keep:\n",
    "        return True, v3_reason, False, \"\"\n",
    "    \n",
    "    # Step 2: V3 rejected it. Try rescue rules.\n",
    "    \n",
    "    # Rescue Rule 1: Very blurry + high animal\n",
    "    if blur_score < RESCUE1_BLUR_MAX:\n",
    "        if animal_score >= RESCUE1_ANIMAL_MIN:\n",
    "            if photo_score >= RESCUE1_PHOTO_MIN:\n",
    "                return True, \"RESCUED: very blurry with high animal\", True, f\"blur={blur_score:.0f}, animal={animal_score:.2f}\"\n",
    "    \n",
    "    # Rescue Rule 2: Multiple animals fallback\n",
    "    if filepath is not None:\n",
    "        if photo_score >= RESCUE2_PHOTO_MIN and animal_score < RESCUE2_ANIMAL_MAX:\n",
    "            multiple_score, single_score = check_multiple_animals(filepath)\n",
    "            \n",
    "            if multiple_score >= RESCUE2_MULTIPLE_MIN or single_score >= RESCUE2_MULTIPLE_MIN:\n",
    "                return True, \"RESCUED: multiple animals detected\", True, f\"multi={multiple_score:.2f}, single={single_score:.2f}\"\n",
    "    \n",
    "    # Step 3: No rescue applied\n",
    "    return False, v3_reason, False, \"\"\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# PATHS\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "data_path = Path(r\"C:\\AWrk\\cats_dogs_project\\data\\PetImages\")\n",
    "output_dir = Path(r\"C:\\AWrk\\cats_dogs_project\\outputs\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "exclude_list_path = output_dir / \"clip_exclude_list_v3plus.txt\"\n",
    "exclude_details_path = output_dir / \"clip_exclude_details_v3plus.txt\"\n",
    "all_images_path = output_dir / \"clip_all_images_v3plus.txt\"\n",
    "rescued_path = output_dir / \"clip_rescued_images_v3plus.txt\"\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# LOAD DATASET\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "cat_files = list((data_path / \"Cat\").glob(\"*.jpg\"))\n",
    "dog_files = list((data_path / \"Dog\").glob(\"*.jpg\"))\n",
    "all_files = cat_files + dog_files\n",
    "\n",
    "print(\"=\" * 110)\n",
    "print(\"FULL PETIMAGES DATASET - V3+ ALGORITHM\")\n",
    "print(\"=\" * 110)\n",
    "print(f\"Total images to process: {len(all_files)}\")\n",
    "print(f\"  Cats: {len(cat_files)}\")\n",
    "print(f\"  Dogs: {len(dog_files)}\")\n",
    "print()\n",
    "print(\"V3+ Design: V3 decisions preserved, rescue rules can only SAVE rejected images\")\n",
    "print()\n",
    "print(\"Rescue Rules:\")\n",
    "print(f\"  1. Very blurry + high animal: blur < {RESCUE1_BLUR_MAX}, animal >= {RESCUE1_ANIMAL_MIN}, photo >= {RESCUE1_PHOTO_MIN}\")\n",
    "print(f\"  2. Multiple animals fallback: photo >= {RESCUE2_PHOTO_MIN}, animal < {RESCUE2_ANIMAL_MAX}, then CLIP check\")\n",
    "print()\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# PROCESS ALL IMAGES\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "kept = []\n",
    "rejected = []\n",
    "rescued = []\n",
    "errors = []\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "print(\"Processing images...\")\n",
    "print(\"-\" * 110)\n",
    "\n",
    "with open(exclude_details_path, \"w\") as details_file, \\\n",
    "     open(all_images_path, \"w\") as all_file, \\\n",
    "     open(rescued_path, \"w\") as rescued_file:\n",
    "    \n",
    "    # Write headers\n",
    "    details_file.write(\"# CLIP Filter V3+ Results - Rejected Images Only\\n\")\n",
    "    details_file.write(\"# path\\tphoto\\tanimal\\ttext\\tblur\\treason\\n\")\n",
    "    details_file.write(\"-\" * 110 + \"\\n\")\n",
    "    \n",
    "    all_file.write(\"# CLIP Filter V3+ Results - All Images\\n\")\n",
    "    all_file.write(\"# path\\tstatus\\tphoto\\tanimal\\ttext\\tblur\\treason\\trescue_details\\n\")\n",
    "    all_file.write(\"-\" * 130 + \"\\n\")\n",
    "    \n",
    "    rescued_file.write(\"# CLIP Filter V3+ Results - Rescued Images\\n\")\n",
    "    rescued_file.write(\"# These images would have been REJECTED by V3 but were SAVED by rescue rules\\n\")\n",
    "    rescued_file.write(\"# path\\tphoto\\tanimal\\ttext\\tblur\\treason\\trescue_details\\n\")\n",
    "    rescued_file.write(\"-\" * 130 + \"\\n\")\n",
    "    \n",
    "    for i, f in enumerate(all_files):\n",
    "        # Relative path format matching existing exclude_list.txt\n",
    "        rel_path = f\"..\\\\data\\\\PetImages\\\\{f.parent.name}\\\\{f.name}\"\n",
    "        \n",
    "        try:\n",
    "            photo, animal, text = check_image_multi(f)\n",
    "            blur = get_blur_score(f)\n",
    "            keep, reason, was_rescued, rescue_details = should_keep_v3plus(photo, animal, text, blur, f)\n",
    "            \n",
    "            status = \"KEEP\" if keep else \"REJECT\"\n",
    "            if was_rescued:\n",
    "                status = \"RESCUED\"\n",
    "            \n",
    "            # Write to all images file\n",
    "            all_file.write(f\"{rel_path}\\t{status}\\t{photo:.2f}\\t{animal:.2f}\\t{text:.2f}\\t{blur:.1f}\\t{reason}\\t{rescue_details}\\n\")\n",
    "            all_file.flush()  # Force write to disk immediately\n",
    "            \n",
    "            if keep:\n",
    "                kept.append((rel_path, photo, animal, text, blur, reason, was_rescued, rescue_details))\n",
    "                if was_rescued:\n",
    "                    rescued.append((rel_path, photo, animal, text, blur, reason, rescue_details))\n",
    "                    rescued_file.write(f\"{rel_path}\\t{photo:.2f}\\t{animal:.2f}\\t{text:.2f}\\t{blur:.1f}\\t{reason}\\t{rescue_details}\\n\")\n",
    "                    rescued_file.flush()  # Force write to disk immediately\n",
    "            else:\n",
    "                rejected.append((rel_path, photo, animal, text, blur, reason))\n",
    "                details_file.write(f\"{rel_path}\\t{photo:.2f}\\t{animal:.2f}\\t{text:.2f}\\t{blur:.1f}\\t{reason}\\n\")\n",
    "                details_file.flush()  # Force write to disk immediately\n",
    "            \n",
    "        except Exception as e:\n",
    "            errors.append((rel_path, str(e)))\n",
    "            rejected.append((rel_path, 0, 0, 0, 0, f\"ERROR: {e}\"))\n",
    "            details_file.write(f\"{rel_path}\\tERROR\\t\\t\\t\\t{e}\\n\")\n",
    "            all_file.write(f\"{rel_path}\\tERROR\\t\\t\\t\\t\\t{e}\\t\\n\")\n",
    "        \n",
    "        # Progress update every 500 images\n",
    "        if (i + 1) % 500 == 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            rate = (i + 1) / elapsed\n",
    "            remaining = (len(all_files) - i - 1) / rate\n",
    "            print(f\"  {i + 1}/{len(all_files)} ({(i+1)/len(all_files)*100:.1f}%) - \"\n",
    "                  f\"Kept: {len(kept)} (rescued: {len(rescued)}) - \"\n",
    "                  f\"Rejected: {len(rejected)} - \"\n",
    "                  f\"ETA: {remaining/60:.1f} min\")\n",
    "\n",
    "# Write simple exclude list (just paths)\n",
    "with open(exclude_list_path, \"w\") as f:\n",
    "    for rel_path, photo, animal, text, blur, reason in rejected:\n",
    "        f.write(rel_path + \"\\n\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# FINAL SUMMARY\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "print()\n",
    "print(\"=\" * 110)\n",
    "print(\"COMPLETE - V3+ ALGORITHM\")\n",
    "print(\"=\" * 110)\n",
    "print(f\"Time elapsed: {elapsed/60:.1f} minutes\")\n",
    "print(f\"Total processed: {len(all_files)}\")\n",
    "print()\n",
    "print(f\"KEPT:     {len(kept)}\")\n",
    "print(f\"  - By V3 rules:    {len(kept) - len(rescued)}\")\n",
    "print(f\"  - By rescue rules: {len(rescued)}\")\n",
    "print(f\"REJECTED: {len(rejected)}\")\n",
    "print(f\"ERRORS:   {len(errors)}\")\n",
    "\n",
    "# Breakdown by class\n",
    "rejected_cats = sum(1 for r in rejected if \"\\\\Cat\\\\\" in r[0])\n",
    "rejected_dogs = sum(1 for r in rejected if \"\\\\Dog\\\\\" in r[0])\n",
    "rescued_cats = sum(1 for r in rescued if \"\\\\Cat\\\\\" in r[0])\n",
    "rescued_dogs = sum(1 for r in rescued if \"\\\\Dog\\\\\" in r[0])\n",
    "\n",
    "print()\n",
    "print(\"By class:\")\n",
    "print(f\"  Rejected Cats: {rejected_cats}\")\n",
    "print(f\"  Rejected Dogs: {rejected_dogs}\")\n",
    "print(f\"  Rescued Cats:  {rescued_cats}\")\n",
    "print(f\"  Rescued Dogs:  {rescued_dogs}\")\n",
    "\n",
    "# Breakdown by reason\n",
    "print()\n",
    "print(\"Rejection reasons:\")\n",
    "reason_counts = {}\n",
    "for item in rejected:\n",
    "    reason = item[5] if len(item) > 5 else \"unknown\"\n",
    "    reason_counts[reason] = reason_counts.get(reason, 0) + 1\n",
    "for reason, count in sorted(reason_counts.items(), key=lambda x: -x[1]):\n",
    "    print(f\"  {count:5d}  {reason}\")\n",
    "\n",
    "# Rescue breakdown\n",
    "print()\n",
    "print(\"Rescue reasons:\")\n",
    "rescue_counts = {}\n",
    "for _, _, _, _, _, reason, _ in rescued:\n",
    "    rescue_counts[reason] = rescue_counts.get(reason, 0) + 1\n",
    "for reason, count in sorted(rescue_counts.items(), key=lambda x: -x[1]):\n",
    "    print(f\"  {count:5d}  {reason}\")\n",
    "\n",
    "# Output files\n",
    "print()\n",
    "print(\"Output files:\")\n",
    "print(f\"  1. {exclude_list_path}\")\n",
    "print(f\"     Paths only (for loading into training)\")\n",
    "print()\n",
    "print(f\"  2. {exclude_details_path}\")\n",
    "print(f\"     Rejected images with scores and reasons\")\n",
    "print()\n",
    "print(f\"  3. {all_images_path}\")\n",
    "print(f\"     ALL images with status, scores, and reasons\")\n",
    "print()\n",
    "print(f\"  4. {rescued_path}\")\n",
    "print(f\"     Images saved by rescue rules (would have been rejected by V3)\")\n",
    "\n",
    "# Show rescued images for review\n",
    "if rescued:\n",
    "    print()\n",
    "    print(f\"RESCUED IMAGES ({len(rescued)} total) - First 30:\")\n",
    "    print(\"-\" * 130)\n",
    "    print(f\"{'path':<50} {'photo':>5} {'animal':>6} {'text':>5} {'blur':>8}  {'reason':<40} rescue_details\")\n",
    "    print(\"-\" * 130)\n",
    "    for rel_path, photo, animal, text, blur, reason, rescue_details in rescued[:30]:\n",
    "        short_path = rel_path.replace(\"..\\\\data\\\\PetImages\\\\\", \"\")\n",
    "        print(f\"{short_path:<50} {photo:>5.2f} {animal:>6.2f} {text:>5.2f} {blur:>8.1f}  {reason:<40} {rescue_details}\")\n",
    "    if len(rescued) > 30:\n",
    "        print(f\"... and {len(rescued) - 30} more (see {rescued_path})\")\n",
    "\n",
    "# Show first 20 rejected for quick review\n",
    "print()\n",
    "print(\"First 20 rejected images:\")\n",
    "print(\"-\" * 110)\n",
    "print(f\"{'path':<50} {'photo':>5} {'animal':>6} {'text':>5} {'blur':>8}  reason\")\n",
    "print(\"-\" * 110)\n",
    "for item in rejected[:20]:\n",
    "    rel_path = item[0]\n",
    "    photo, animal, text, blur = item[1], item[2], item[3], item[4]\n",
    "    reason = item[5] if len(item) > 5 else \"unknown\"\n",
    "    short_path = rel_path.replace(\"..\\\\data\\\\PetImages\\\\\", \"\")\n",
    "    print(f\"{short_path:<50} {photo:>5.2f} {animal:>6.2f} {text:>5.2f} {blur:>8.1f}  {reason}\")\n",
    "\n",
    "if errors:\n",
    "    print()\n",
    "    print(f\"Errors ({len(errors)}):\")\n",
    "    for path, err in errors[:10]:\n",
    "        print(f\"  {path}: {err}\")\n",
    "\n",
    "print()\n",
    "print(\"=\" * 110)\n",
    "print(\"V3+ ALGORITHM SUMMARY\")\n",
    "print(\"=\" * 110)\n",
    "print(\"Design principle: If V3 keeps it â†’ V3+ keeps it. Always.\")\n",
    "print(\"Rescue rules can ONLY save images that V3 would reject.\")\n",
    "print()\n",
    "print(\"Rescue Rule 1: Very blurry + high animal\")\n",
    "print(f\"  blur < {RESCUE1_BLUR_MAX}, animal >= {RESCUE1_ANIMAL_MIN}, photo >= {RESCUE1_PHOTO_MIN}\")\n",
    "print(\"  Catches: yawning cats, unusual poses, solid backgrounds\")\n",
    "print()\n",
    "print(\"Rescue Rule 2: Multiple animals fallback\")\n",
    "print(f\"  photo >= {RESCUE2_PHOTO_MIN}, animal < {RESCUE2_ANIMAL_MAX}\")\n",
    "print(f\"  Secondary CLIP check, threshold >= {RESCUE2_MULTIPLE_MIN}\")\n",
    "print(\"  Catches: group photos, cats in cages, multiple kittens\")\n",
    "print(\"=\" * 110)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "catdog",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
